.. _RBM:

Restricted Boltzmann Machines (RBM)
===================================

Energy-Based Models (EBM)
+++++++++++++++++++++++++

**Energy-based** models associate a scalar energy to each configuration of the
variables of interest. Learning corresponds to modifying that energy function
so that its shape has desirable properties. For example, we would like
plausible or desirable configurations to have low energy.  Energy-based
probabilistic models define a probability distribution through an energy
function, as follows:

.. math::
  :label: energy1

  p(x) = \frac {e^{-E(x)}} {Z}.

The normalizing factor :math:`Z` is called the **partition function** by analogy
with physical systems.

.. math::
    Z = \sum_x e^{-E(x)}

An energy-based model can be learnt by performing (stochastic) gradient
descent on the empirical log-likelihood of the training data:

.. math::
    \mathcal{L}(\theta, \mathcal{D}) = \frac{1}{N} \sum_{x^{(i)} \in
    \mathcal{D}} log\ p(x^{(i)}).

using the stochastic gradient :math:`\frac{\partial p(x^{(i)})}{\partial
\theta}`, where :math:`\theta` are the parameters of the model.


**EBMs with Hidden Units**

In many cases of interest, we do not observe the example :math:`x` fully, or we
want to introduce some non-observed variables to increase the expressive power
of the model. So we consider an observed part (still denoted :math:`x` here) and a
**hidden** part :math:`h`. We can then write:

.. math::
  :label: energy2

   P(x) = \sum_h P(x,h) = \sum_h \frac{e^{-E(x,h)}}{Z}.

In such cases, to map this formulation to one similar to Eq. :eq:`energy1`, we
introduce the notation (inspired from physics) of **free energy**, defined as
follows:

.. math::
    &P(x) = \frac{e^{-\mathcal{F}(x)}}{Z}, \\
    &\text{with } \mathcal{F}(x) = - \log \sum_h e^{-E(x,h)}
    \text{ and } Z=\sum_x e^{-\mathcal{F}(x)}.

The data log-likelihood gradient then has a particularly interesting form.

.. math::
  \frac{\partial log p(x)}{\partial \theta}
   &= - \frac{\partial \mathcal{F}(x)}{\partial \theta} + 
         \sum_{\tilde{x}} p(\tilde{x}) \
             \frac{\partial \mathcal{F}(\tilde{x})}{\partial \theta}.

Notice that the above gradient contains two terms, which are referred to as
the **positive** and **negative phase**. The terms positive and negative do
not refer to the sign of each term in the equation, but rather reflect their
effect on the energy landscape. The first term increases the probability of
training data (by reducing the corresponding free energy), while the second
term decreases the probability of samples generated by the model.

It is usually difficult to determine this gradient analytically, as it
involves the computation of 
:math:`E_P [ \frac{\partial \mathcal{F}(x)} {\partial \theta} ]`, which is
nothing less than the expectation of the derivative of the free energy for all
possible inputs :math:`x` (under the distribution :math:`P` formed by the
model).

The first step in making this computation tractable is to replace the
expectation with an estimate over a mini-batch of model samples. Doing this
requires a robust sampling algorithm. While many exist in the litterature, we
will see how Markov Chain Monte Carlo methods are especially well suited for
models such as the Restricted Boltzmann Machines (RBM), a specific type of
EBM.


Restricted Boltzmann Machines (RBM)
+++++++++++++++++++++++++++++++++++

Boltzmann Machines (BMs) are a particular form of energy-based model which
contain hidden variables. Restricted Boltzmann Machines further restrict BMs
to those without visible-visible and hidden-hidden connections.

The energy function :math:`E(v,h)` of an RBM is defined as:

.. math::
    :label: rbm_energy

    E(v,h) = - b'v - c'h - h'Wv 

where :math:`W` represents the weights connecting hidden and visible units and
:math:`b`, :math:`c` are the offsets of the visible and hidden layers
respectively. A graphical depiction of an RBM is shown below.

.. image:: images/rbm.png
    :align: center

Because of the specific structure of RBMs, visible and hidden units are
conditionally independent given one-another. Using this property, along with
Eqs. :eq:`energy2` and :eq:`rbm_energy`, allows us to write:

.. math::
    p(h|v) &= \prod_i p(h_i|v) = \prod_i sigm(Wv + c) \\
    p(v|h) &= \prod_j p(v_j|h) = \prod_j sigm(W'h + b).


Sampling Negative Particles
+++++++++++++++++++++++++++

Samples of :math:`p(v)` can be obtained by running a Markov chain to
convergence, using Gibbs sampling as the transition operator.

Gibbs sampling of the joint of N random variables :math:`S=(S_1, ... , S_N)`
is done through a sequence of N sampling sub-steps of the form 
:math:`S_i \sim p(S_i | S_{-i})` where :math:`S_{-i}` contains the :math:`N-1`
other random variables in :math:`S` excluding :math:`S_i`.

For RBMs, :math:`S` consists of the set of visible and hidden units. However,
since they are conditionally independent, one can perform block Gibbs
sampling. In this setting, visible units can be sampled simultaneously given
fixed values for the hidden units, and vice-versa. A step in the Markov chain
is thus taken as follows: 

.. math::
    h^{(n+1)} &\sim sigm(W'v^{(n)} + c) \\
    v^{(n+1)} &\sim sigm(W h^{(n+1)} + b).

This can be illustrated graphically:

.. image:: images/markov_chain.png
    :align: center

In the figure above, the superscript refers to the index of the sample within
the Markov chain. As :math:`t \rightarrow \infty`, samples :math:`(v^{(t)},
h^{(t)})` are guaranteed to be accurate samples of :math:`p(v,h)`.

In theory, each parameter update in the learning process would require running
one such chain to convergence. It is needless to say that doing so would be
prohibitively expensive. As such, several algorithms have been devised for
RBMs, in order to efficient sample from :math:`p(v,h)` during the learning
process.


Contrastive Divergence
----------------------

Contrastive Divergence uses two tricks to speed up the sampling process:

* since we eventually want :math:`p(v) \approx p_T(v)` (the true, underlying
  distribution of the data), we initialize the Markov chain with a training
  example. This should help speed up convergence of the chain.
  
* CD does not wait for the chain to converge. Samples are obtained after only
  k-steps of Gibbs sampling. In pratice, :math:`k=1` has been shown to work
  surprisingly well.


Persistent CD
-------------

Persistent CD [Tieleman08]_ uses another approximation for sampling from
:math:`p(v,h)`.  It relies on a single Markov chain, which has a persistent
state. For each parameter update, we extract new samples by simply running the
chain for k-steps. The state of the chain is then preserved for subsequent updates.

The general intuition is that if parameter updates are small enough compared
to the mixing rate of the chain, the Markov chain should be able to "catch up"
to changes in the model.



References
++++++++++

.. [Bengio09] Y. Bengio, `Learning deep architectures for AI <http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/239>`_, Foundations and Trends in Machine Learning 1(2) pages 1-127.

.. [Tieleman08] T. Tieleman, `Training restricted boltzmann machines using approximations to the likelihood gradient`, ICML 2008.
