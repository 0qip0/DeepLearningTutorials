.. _RBM:

Restricted Boltzmann Machines (RBM)
===================================

Energy-Based Models (EBM)
+++++++++++++++++++++++++

**Energy-based** models associate a scalar energy to each configuration of the
variables of interest. Learning corresponds to modifying that energy function
so that its shape has desirable properties. For example, we would like
plausible or desirable configurations to have low energy.  Energy-based
probabilistic models define a probability distribution through an energy
function, as follows:

.. math::
  :label: energy1

  p(x) = \frac {e^{-E(x)}} {Z}.

The normalizing factor :math:`Z` is called the **partition function** by analogy
with physical systems.

.. math::
    Z = \sum_x e^{-E(x)}

An energy-based model can be learnt by performing (stochastic) gradient
descent on the empirical log-likelihood of the training data:

.. math::
    \mathcal{L}(\theta, \mathcal{D}) = \frac{1}{N} \sum_{x^{(i)} \in
    \mathcal{D}} log\ p(x^{(i)}).

using the stochastic gradient :math:`\frac{\partial p(x^{(i)})}{\partial
\theta}`, where :math:`\theta` are the parameters of the model.


**EBMs with Hidden Units**

In many cases of interest, we do not observe the example :math:`x` fully, or we
want to introduce some non-observed variables to increase the expressive power
of the model. So we consider an observed part (still denoted :math:`x` here) and a
**hidden** part :math:`h`. We can then write:

.. math::
  :label: energy2

   P(x) = \sum_h P(x,h) = \sum_h \frac{e^{-E(x,h)}}{Z}.

In such cases, to map this formulation to one similar to Eq. :eq:`energy1`, we
introduce the notation (inspired from physics) of **free energy**, defined as
follows:

.. math::
   :label: free_energy

    \mathcal{F}(x) = - \log \sum_h e^{-E(x,h)}

which allows us to write,

.. math::
    &P(x) = \frac{e^{-\mathcal{F}(x)}}{Z} \text{ with } Z=\sum_x e^{-\mathcal{F}(x)}.

The data log-likelihood gradient then has a particularly interesting form.

.. math::
  \frac{\partial log p(x)}{\partial \theta}
   &= - \frac{\partial \mathcal{F}(x)}{\partial \theta} + 
         \sum_{\tilde{x}} p(\tilde{x}) \
             \frac{\partial \mathcal{F}(\tilde{x})}{\partial \theta}.

Notice that the above gradient contains two terms, which are referred to as
the **positive** and **negative phase**. The terms positive and negative do
not refer to the sign of each term in the equation, but rather reflect their
effect on the probability density defined by the model. The first term
increases the probability of training data (by reducing the corresponding free
energy), while the second term decreases the probability of samples generated
by the model.

It is usually difficult to determine this gradient analytically, as it
involves the computation of 
:math:`E_P [ \frac{\partial \mathcal{F}(x)} {\partial \theta} ]`. This is
nothing less than an expectation over all possible configurations of the input
:math:`x` (under the distribution :math:`P` formed by the model) !

The first step in making this computation tractable is to estimate the
expectation using a fixed number of model samples. Samples used to estimate the
negative phase gradient are referred to as **negative particles**, which are
denoted as :math:`\mathcal{N}`. The gradient can then be written as:

.. math::
  \frac{\partial log p(x)}{\partial \theta}
   &\approx 
   - \frac{\partial \mathcal{F}(x)}{\partial \theta} + 
     \sum_{\tilde{x} \in \mathcal{N}} p(\tilde{x}) \
     \frac{\partial \mathcal{F}(\tilde{x})}{\partial \theta}.

With the above formula, we almost have a pratical, stochastic algorithm for
learning an EBM. The only missing ingredient is how to extract these negative
particles :math:`\mathcal{N}`. While the statistical litterature abounds with
sampling methods, Markov Chain Monte Carlo methods are especially well suited
for models such as the Restricted Boltzmann Machines (RBM), a specific type of
EBM.


Restricted Boltzmann Machines (RBM)
+++++++++++++++++++++++++++++++++++

Boltzmann Machines (BMs) are a particular form of energy-based model which
contain hidden variables. Restricted Boltzmann Machines further restrict BMs to
those without visible-visible and hidden-hidden connections.  A graphical
depiction of an RBM is shown below.

.. image:: images/rbm.png
    :align: center

The energy function :math:`E(x,h)` of an RBM is defined as:

.. math::
    :label: rbm_energy

    E(v,h) = - b'x - c'h - h'Wx 

where :math:`W` represents the weights connecting hidden and visible units and
:math:`b`, :math:`c` are the offsets of the visible and hidden layers
respectively.

This translates directly to the following free energy formula:

.. math::
  \mathcal{F}(x)= - b'x - \sum_i log \sum_{h_i} e^{h_i (c_i + W_i x)}.

Because of the specific structure of RBMs, visible and hidden units are
conditionally independent given one-another. Using this property, we can
write:

.. math::
    p(h|x) &= \prod_i p(h_i|x) \\
    p(x|h) &= \prod_j p(x_j|h).

**RBMs with binary units**

In the commonly studied case of using binary units (where :math:`h_i \in
\{0,1\}`, we obtain from Eq. :eq:`rbm_energy` and :eq:`energy2`, a stochastic
version of the usual neuron activation function:

.. math::
  P(h_i=1|x) = sigm(c_i + W_i x) \\
  P(x_j=1|h) = sigm(b_j + W'_j h)

The free energy of an RBM with binary units further simplifies to:

.. math::
  \mathcal{F}(x)= - b'x - \sum_i log 1 + e^{(c_i + W_i x)}.


Sampling in an RBM
++++++++++++++++++

Samples of :math:`p(x)` can be obtained by running a Markov chain to
convergence, using Gibbs sampling as the transition operator.

Gibbs sampling of the joint of N random variables :math:`S=(S_1, ... , S_N)`
is done through a sequence of N sampling sub-steps of the form 
:math:`S_i \sim p(S_i | S_{-i})` where :math:`S_{-i}` contains the :math:`N-1`
other random variables in :math:`S` excluding :math:`S_i`.

For RBMs, :math:`S` consists of the set of visible and hidden units. However,
since they are conditionally independent, one can perform block Gibbs
sampling. In this setting, visible units are sampled simultaneously given
fixed values of the hidden units. Similarly, hidden units are sampled
simultaneously given the visibles. A step in the Markov chain is thus taken as
follows: 

.. math::
    h^{(n+1)} &\sim sigm(W'x^{(n)} + c) \\
    x^{(n+1)} &\sim sigm(W h^{(n+1)} + b),

where :math:`h^{(n)}` refers to the set of all hidden units at the n-th step of
within the Markov chain.

This can be illustrated graphically:

.. image:: images/markov_chain.png
    :align: center

As :math:`t \rightarrow \infty`, samples :math:`(x^{(t)}, h^{(t)})` are
guaranteed to be accurate samples of :math:`p(x,h)`.

In theory, each parameter update in the learning process would require running
one such chain to convergence. It is needless to say that doing so would be
prohibitively expensive. As such, several algorithms have been devised for
RBMs, in order to efficiently sample from :math:`p(x,h)` during the learning
process.


Contrastive Divergence (CD-k)
-----------------------------

Contrastive Divergence uses two tricks to speed up the sampling process:

* since we eventually want :math:`p(x) \approx p_T(x)` (the true, underlying
  distribution of the data), we initialize the Markov chain with a training
  example.
  
* CD does not wait for the chain to converge. Samples are obtained after only
  k-steps of Gibbs sampling. In pratice, :math:`k=1` has been shown to work
  surprisingly well.


Persistent CD
-------------

Persistent CD [Tieleman08]_ uses another approximation for sampling from
:math:`p(x,h)`.  It relies on a single Markov chain, which has a persistent
state. For each parameter update, we extract new samples by simply running the
chain for k-steps. The state of the chain is then preserved for subsequent updates.

The general intuition is that if parameter updates are small enough compared
to the mixing rate of the chain, the Markov chain should be able to "catch up"
to changes in the model.

