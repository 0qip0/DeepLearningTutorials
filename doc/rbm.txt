.. _RBM:

Restricted Boltzmann Machines (RBM)
===================================


.. note::
    The code for this section is available for download `here`_.

.. _here: http://deeplearning.net/tutorial/code/rbm.py



Energy-Based Models (EBM)
+++++++++++++++++++++++++

**Energy-based** models associate a scalar energy to each configuration of the
variables of interest. Learning corresponds to modifying that energy function
so that its shape has desirable properties. For example, we would like
plausible or desirable configurations to have low energy.  Energy-based
probabilistic models define a probability distribution through an energy
function, as follows:

.. math::
  :label: energy1

  p(x) = \frac {e^{-E(x)}} {Z}.

The normalizing factor :math:`Z` is called the **partition function** by analogy
with physical systems.

.. math::
    Z = \sum_x e^{-E(x)}

An energy-based model can be learnt by performing (stochastic) gradient
descent on the empirical log-likelihood of the training data:

.. math::
    \mathcal{L}(\theta, \mathcal{D}) = \frac{1}{N} \sum_{x^{(i)} \in
    \mathcal{D}} log\ p(x^{(i)}).

using the stochastic gradient :math:`\frac{\partial p(x^{(i)})}{\partial
\theta}`, where :math:`\theta` are the parameters of the model.


**EBMs with Hidden Units**

In many cases of interest, we do not observe the example :math:`x` fully, or we
want to introduce some non-observed variables to increase the expressive power
of the model. So we consider an observed part (still denoted :math:`x` here) and a
**hidden** part :math:`h`. We can then write:

.. math::
  :label: energy2

   P(x) = \sum_h P(x,h) = \sum_h \frac{e^{-E(x,h)}}{Z}.

In such cases, to map this formulation to one similar to Eq. :eq:`energy1`, we
introduce the notation (inspired from physics) of **free energy**, defined as
follows:

.. math::
   :label: free_energy

    \mathcal{F}(x) = - \log \sum_h e^{-E(x,h)}

which allows us to write,

.. math::
    &P(x) = \frac{e^{-\mathcal{F}(x)}}{Z} \text{ with } Z=\sum_x e^{-\mathcal{F}(x)}.

The data log-likelihood gradient then has a particularly interesting form.

.. math::
  \frac{\partial log p(x)}{\partial \theta}
   &= - \frac{\partial \mathcal{F}(x)}{\partial \theta} + 
         \sum_{\tilde{x}} p(\tilde{x}) \
             \frac{\partial \mathcal{F}(\tilde{x})}{\partial \theta}.

Notice that the above gradient contains two terms, which are referred to as
the **positive** and **negative phase**. The terms positive and negative do
not refer to the sign of each term in the equation, but rather reflect their
effect on the probability density defined by the model. The first term
increases the probability of training data (by reducing the corresponding free
energy), while the second term decreases the probability of samples generated
by the model.

It is usually difficult to determine this gradient analytically, as it
involves the computation of 
:math:`E_P [ \frac{\partial \mathcal{F}(x)} {\partial \theta} ]`. This is
nothing less than an expectation over all possible configurations of the input
:math:`x` (under the distribution :math:`P` formed by the model) !

The first step in making this computation tractable is to estimate the
expectation using a fixed number of model samples. Samples used to estimate the
negative phase gradient are referred to as **negative particles**, which are
denoted as :math:`\mathcal{N}`. The gradient can then be written as:

.. math::
  :label: bm_grad

  \frac{\partial log p(x)}{\partial \theta}
   &\approx 
   - \frac{\partial \mathcal{F}(x)}{\partial \theta} + 
     \sum_{\tilde{x} \in \mathcal{N}} p(\tilde{x}) \
     \frac{\partial \mathcal{F}(\tilde{x})}{\partial \theta}.

With the above formula, we almost have a pratical, stochastic algorithm for
learning an EBM. The only missing ingredient is how to extract these negative
particles :math:`\mathcal{N}`. While the statistical litterature abounds with
sampling methods, Markov Chain Monte Carlo methods are especially well suited
for models such as the Restricted Boltzmann Machines (RBM), a specific type of
EBM.


Restricted Boltzmann Machines (RBM)
+++++++++++++++++++++++++++++++++++

Boltzmann Machines (BMs) are a particular form of energy-based model which
contain hidden variables. Restricted Boltzmann Machines further restrict BMs to
those without visible-visible and hidden-hidden connections.  A graphical
depiction of an RBM is shown below.

.. image:: images/rbm.png
    :align: center

The energy function :math:`E(x,h)` of an RBM is defined as:

.. math::
    :label: rbm_energy

    E(v,h) = - b'x - c'h - h'Wx 

where :math:`W` represents the weights connecting hidden and visible units and
:math:`b`, :math:`c` are the offsets of the visible and hidden layers
respectively.

This translates directly to the following free energy formula:

.. math::

  \mathcal{F}(x)= - b'x - \sum_i log \sum_{h_i} e^{h_i (c_i + W_i x)}.

Because of the specific structure of RBMs, visible and hidden units are
conditionally independent given one-another. Using this property, we can
write:

.. math::
    p(h|x) &= \prod_i p(h_i|x) \\
    p(x|h) &= \prod_j p(x_j|h).

**RBMs with binary units**

In the commonly studied case of using binary units (where :math:`h_i \in
\{0,1\}`, we obtain from Eq. :eq:`rbm_energy` and :eq:`energy2`, a stochastic
version of the usual neuron activation function:

.. math::
  :label: rbm_propup

  P(h_i=1|x) = sigm(c_i + W_i x) \\

.. math::
  :label: rbm_propdown

  P(x_j=1|h) = sigm(b_j + W'_j h)

The free energy of an RBM with binary units further simplifies to:

.. math::
  :label: rbm_free_energy   

  \mathcal{F}(x)= - b'x - \sum_i log 1 + e^{(c_i + W_i x)}.

**Update Equations with Binary Units**

Combining Eqs. :eq:`bm_grad` with :eq:`rbm_free_energy`, we obtain the
following log-likelihood gradients for an RBM with binary units:

.. math::
    :label: rbm_grad

    \frac {\partial{log p(v)}} {\partial W_{ij}} &= 
       - x^{(i)}_j \cdot sigm(W_i \cdot x^{(i)} + c_i)
       + E_v[p(h_i|v) \cdot v_j] \\
    \frac {\partial{log p(v)}} {\partial c_i} &=
       - sigm(W_i \cdot x^{(i)}) + E_v[p(h_i|v)] \\
    \frac {\partial{log p(v)}} {\partial b_j} &=
       - x^{(i)} + E_v[p(v_j|h)]

.. note::
    We will be updating the tutorial shortly, such that the gradients are
    directly computed (using ``T.grad``) from the free energy formula.


Sampling in an RBM
++++++++++++++++++

Samples of :math:`p(x)` can be obtained by running a Markov chain to
convergence, using Gibbs sampling as the transition operator.

Gibbs sampling of the joint of N random variables :math:`S=(S_1, ... , S_N)`
is done through a sequence of N sampling sub-steps of the form 
:math:`S_i \sim p(S_i | S_{-i})` where :math:`S_{-i}` contains the :math:`N-1`
other random variables in :math:`S` excluding :math:`S_i`.

For RBMs, :math:`S` consists of the set of visible and hidden units. However,
since they are conditionally independent, one can perform block Gibbs
sampling. In this setting, visible units are sampled simultaneously given
fixed values of the hidden units. Similarly, hidden units are sampled
simultaneously given the visibles. A step in the Markov chain is thus taken as
follows: 

.. math::
    h^{(n+1)} &\sim sigm(W'x^{(n)} + c) \\
    x^{(n+1)} &\sim sigm(W h^{(n+1)} + b),

where :math:`h^{(n)}` refers to the set of all hidden units at the n-th step of
within the Markov chain.

This can be illustrated graphically:

.. image:: images/markov_chain.png
    :align: center

As :math:`t \rightarrow \infty`, samples :math:`(x^{(t)}, h^{(t)})` are
guaranteed to be accurate samples of :math:`p(x,h)`.

In theory, each parameter update in the learning process would require running
one such chain to convergence. It is needless to say that doing so would be
prohibitively expensive. As such, several algorithms have been devised for
RBMs, in order to efficiently sample from :math:`p(x,h)` during the learning
process.


Contrastive Divergence (CD-k)
-----------------------------

Contrastive Divergence uses two tricks to speed up the sampling process:

* since we eventually want :math:`p(x) \approx p_T(x)` (the true, underlying
  distribution of the data), we initialize the Markov chain with a training
  example.
  
* CD does not wait for the chain to converge. Samples are obtained after only
  k-steps of Gibbs sampling. In pratice, :math:`k=1` has been shown to work
  surprisingly well.


Persistent CD
-------------

Persistent CD [Tieleman08]_ uses another approximation for sampling from
:math:`p(x,h)`.  It relies on a single Markov chain, which has a persistent
state. For each parameter update, we extract new samples by simply running the
chain for k-steps. The state of the chain is then preserved for subsequent updates.

The general intuition is that if parameter updates are small enough compared
to the mixing rate of the chain, the Markov chain should be able to "catch up"
to changes in the model.


Implementation
++++++++++++++

We construct an ``RBM`` class. The parameters of the network can either be
initialized by the constructor or can be passed as arguments. This option is
useful when a RBM is used as the building block of a deep network, in which
case the weight matrix and the hidden layer bias is shared with the
corresponding sigmoidal layer of a MLP network.

.. code-block:: python

  class RBM(object):
    """Restricted Boltzmann Machine (RBM) """
    def __init__(self, input=None, n_visible=784, n_hidden=500, \
        W = None, hbias = None, vbias = None, numpy_rng = None, 
        theano_rng = None):
        """ 
        RBM constructor. Defines the parameters of the model along with
        basic operations for inferring hidden from visible (and vice-versa), 
        as well as for performing CD updates.

        :param input: None for standalone RBMs or symbolic variable if RBM is
        part of a larger graph.

        :param n_visible: number of visible units

        :param n_hidden: number of hidden units

        :param W: None for standalone RBMs or symbolic variable pointing to a
        shared weight matrix in case RBM is part of a DBN network; in a DBN,
        the weights are shared between RBMs and layers of a MLP

        :param hbias: None for standalone RBMs or symbolic variable pointing 
        to a shared hidden units bias vector in case RBM is part of a 
        different network

        :param vbias: None for standalone RBMs or a symbolic variable 
        pointing to a shared visible units bias
        """

        self.n_visible = n_visible
        self.n_hidden  = n_hidden


        if W is None : 
           # W is initialized with `initial_W` which is uniformely sampled
           # from -6./sqrt(n_visible+n_hidden) and 6./sqrt(n_hidden+n_visible)
           # the output of uniform if converted using asarray to dtype 
           # theano.config.floatX so that the code is runable on GPU
           initial_W = numpy.asarray( numpy.random.uniform( 
                     low = -numpy.sqrt(6./(n_hidden+n_visible)), 
                     high = numpy.sqrt(6./(n_hidden+n_visible)), 
                     size = (n_visible, n_hidden)), 
                     dtype = theano.config.floatX)
           # theano shared variables for weights and biases
           W = theano.shared(value = initial_W, name = 'W')

        if hbias is None :
           # create shared variable for hidden units bias
           hbias = theano.shared(value = numpy.zeros(n_hidden, 
                               dtype = theano.config.floatX), name='hbias')

        if vbias is None :
            # create shared variable for visible units bias
            vbias = theano.shared(value =numpy.zeros(n_visible, 
                                dtype = theano.config.floatX),name='vbias')

        if numpy_rng is None:    
            # create a number generator 
            numpy_rng = numpy.random.RandomState(1234)

        if theano_rng is None : 
            theano_rng = RandomStreams(numpy_rng.randint(2**30))


        # initialize input layer for standalone RBM or layer0 of DBN
        self.input = input if input else T.dmatrix('input')

        self.W          = W
        self.hbias      = hbias
        self.vbias      = vbias
        self.theano_rng = theano_rng
        # **** WARNING: It is not a good idea to put things in this list 
        # other than shared variables created in this function.
        self.params     = [self.W, self.hbias, self.vbias]
        self.batch_size = self.input.shape[0]


Next step is to define a function which defines the symbolic graph associated
with a single step of Gibbs sampling. Since we are going to use only CD-1 in
this tutorial we need to implement just one step of Gibbs sampling. The
following function does this by:

* inferring the activation probabilities of the hidden units, given the
  state `v0_sample` of the visibles (Eq. :eq:`rbm_propup`)
* sampling the hidden units
* inferring the activation probabilities of the visible units, given the state
  `h0_sample` of the hidden units (Eqs. :eq:`rbm_propdown`)
* sampling the visible units

The code is as follows:

.. code-block:: python

    def gibbs_1(self, v0_sample):
        ''' This function implements one step of Gibbs sampling '''
        # compute the activation of the hidden units given a sample of the
        # vissibles
        h0_mean = T.nnet.sigmoid(T.dot(v0_sample, self.W) + self.hbias)
        # get a sample of the hiddens given their activation
        h0_sample = self.theano_rng.binomial(size = h0_mean.shape, n = 1, 
                                                            prob = h0_mean)
        # compute the activation of the visible given the hidden sample
        v1_mean = T.nnet.sigmoid(T.dot(h0_sample, self.W.T) + self.vbias)
        # get a sample of the visible given their activation
        v1_sample = self.theano_rng.binomial(size = v1_mean.shape, n = 1, 
                                                            prob = v1_mean)
        return [v1_mean, v1_sample]
 

We then add a ``cd`` method, whose purpose is to generate the symbolic
gradients for CD-1 and PCD-1 updates.

.. code-block:: python

    def cd(self, lr = 0.1, persistent=None):
        """ This functions implements one step of CD-1

        :param lr: learning rate used to train the RBM

        Return a tuple of values describing a CD step. The first value 
        in the tuple is the reconstruction cross-entropy cost, a proxy 
        to what CD tries to minimize, while the second is an update 
        dictionary. The dictionary contains the update rules for weights
        and biases but also an update of the shared variable used to store
        the persistent chain, if one is used.
        
        Note that the reconstruction cross-entropy cost is just
        a proxy of what CD actually tries to minimize, do not confuse the 
        two.

        If persistent is None, it defaults to self.input

        CD aka CD1 - cd()
        PCD        - cd(persistent=shared(numpy.asarray(initializer)))
        """

        if persistent is None:
            chain_start = self.input
        else:
            chain_start = persistent


Note that ``cd`` takes as argument a variable called ``persistent``. While this
may be confusing to the reader (since CD is by definition **not** persistent),
this little trick allows us to use the same code to implement both CD and PCD.
To use PCD, ``persistent`` should refer to a shared variable which contains the
state of the Gibbs chain from the previous iteration.

If ``persistent`` is ``None``, we initialize the input with a standard
``dmatrix``, which will eventually map to a training example.  Given that we
know the starting point of the chain, we can then compute the values
of the visible and hidden units in both the positive and negative phases.
These are requires to compute the gradient of Eq. :eq:`rbm_grad`.

.. code-block:: python


        # define the graph for positive phase
        ph_act    = T.dot(chain_start, self.W) + self.hbias
        ph_mean   = T.nnet.sigmoid(ph_act)
        ph_sample = self.theano_rng.binomial(size = ph_mean.shape, n = 1,
                                                         prob = ph_mean)

        # define the graph for the negative phase
        nv_act    = T.dot(ph_sample, self.W.T) + self.vbias
        nv_mean   = T.nnet.sigmoid(nv_act)
        nv_sample = self.theano_rng.binomial(size = nv_mean.shape, n = 1,
                                                         prob = nv_mean)
        nh_act    = T.dot(nv_sample, self.W) + self.hbias
        nh_mean   = T.nnet.sigmoid(nh_act)
        nh_sample = self.theano_rng.binomial(size = nh_mean.shape, n = 1,
                                                         prob = nh_mean)


        g_vbias = T.sum( self.input - nv_mean, axis = 0)/self.batch_size
        g_hbias = T.sum( ph_mean    - nh_mean, axis = 0)/self.batch_size
        g_W = T.dot(ph_mean.T, self.input   )/ self.batch_size - \
              T.dot(nh_mean.T, nv_mean      )/ self.batch_size

        gparams = [g_W.T, g_hbias, g_vbias]

Finally, we compute the reconstruction cross-entropy cost. This can be 
seen as a proxy to the function minimized by CD (see [BengioDelalleau09]_).
While this value is not used anywhere, it is a good indication that the RBM is
learning.  We also construct the updates dictionary containing the parameter
updates. In case of PCD, these should also update the shared variable
containing the state of the Gibbs chain.

.. code-block:: python

        cross_entropy = T.mean(T.sum( self.input*T.log(nv_mean) 
                       + (1 - self.input)*T.log(1-nv_mean), axis = 1))

        # constructs the update dictionary
        updates = {}
        for gparam, param in zip(gparams, self.params):
           updates[param] = param + gparam * lr

        if persistent:
            # Note that this works only if persistent is a shared variable
            updates[persistent] = nv_sample


        return (cross_entropy, updates)

We now have all the necessary ingredients to start training our network.

Before going over the training loop however, the reader should familiarize
himself with the function ``tile_raster_images`` (see :ref:`how-to-plot`). Since
RBMs are generative models, we are interested in sampling from them and
plotting/visualizing these samples. We also want to visualize the filters
(weights) learnt by the RBM, to gain insights into what the RBM is actually
doing. Bare in mind however, that this does not provide the entire story,
since we neglect the biases and plot the weights up to a multiplicative
constant (weights are converted to values between 0 and 1). 

Having these utility functions, we can start training the RBM and plot/save
the filters after each training epoch.  We train the RBM using PCD, as it has
been shown to lead to a better generative model ([Tieleman]_).

.. code-block:: python

    for epoch in xrange(training_epochs):
        # go through the training set
        c = []
        for batch_index in xrange(n_train_batches):
           c += [ train_rbm(batch_index) ]
        print 'Training epoch %d, average cross-entropy reconstruction cost '%epoch, numpy.mean(c)
        plotting_start = time.clock()
        #           Plot filters after each training epoch
        # Construct image from the weight matrix 
        image = PIL.Image.fromarray(tile_raster_images( X = rbm.W.value.T,
                 img_shape = (28,28),tile_shape = (10,10), 
                 tile_spacing=(1,1)))
        image.save('filters_at_epoch_%i.png'%epoch) 
        plotting_stop = time.clock()
        plotting_time += (plotting_stop - plotting_start)
    end_time = time.clock()

    pretraining_time = (end_time - start_time) - plotting_time

    print ('Training took %f minutes' %(pretraining_time/60.))



Once the RBM is trained, we can then use the ``gibbs_1`` function to implement
the Gibbs chain required for sampling. We initialize the Gibbs chain starting
from test examples (although we could as well pick it from the training set)
in order to speed up convergence and avoid problems with random
initialization.

.. code-block:: python

    # Sampling the RBM

    # Initialize the persistent chain with some sample from the test 

    # find out the number of test samples  
    number_of_test_samples = test_set_x.value.shape[0]

    # pick one randomly
    sample = rng.randint(number_of_test_samples-20)

    # initialize 20 persistent chains in parallel
    persistent_chain = theano.shared( test_set_x.value[sample:sample+20])

Next we create the 20 persistent chains in paralel to get our
samples. To do so, we compile a theano function which performs one Gibbs step
and updates the state of the persistent chain with the new visible sample. We
apply this function iteratively for a large number of steps, plotting the
samples at every 1000 step.

.. code-block:: python

    # the sample at the end of the channel is returned by ``gibbs_1`` as 
    # its second output; note that this is computed as a binomial draw, 
    # therefore it is formed of ints (0 and 1) and therefore needs to 
    # be converted to the same dtype as ``persistent_chain``
    chain_step_sample = T.cast(rbm.gibbs_1(persistent_chain)[1], dtype =
                                                    theano.config.floatX)
    # construct the function that implements our persistent chain 
    sample_fn = theano.function([], chain_step_sample, 
                      updates = { persistent_chain:chain_step_sample })

    # sample the RBM, plotting every `plot_every`-th sample; do this 
    # until you plot at least `n_samples`
    n_samples = 10
    plot_every = 1000

    for idx in xrange(n_samples):
        # do `plot_every` intermediate samplings of which we do not care
        for jdx in  xrange(plot_every):
            sample_fn()
        # construct image
        image = PIL.Image.fromarray(tile_raster_images( 
                                         X          = persistent_chain.value,
                                         img_shape  = (28,28),
                                         tile_shape = (10,10),
                                         tile_spacing = (1,1) ) )
        print ' ... plotting sample ', idx
        image.save('sample_%i_step_%i.png'%(idx,idx*jdx))


Results
+++++++

Training took 20.862 minutes for 15 epochs with a learning rate 0.1 .
The pictures below shows the filters after 15 epochs : 

.. image:: images/filters_at_epoch_14.png
    :align: center






