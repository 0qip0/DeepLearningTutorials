Convolutional Neural Networks (LeNet)
=====================================

.. note::
    This section assumes the reader has already read through :doc:`logreg` and
    :doc:`mlp`. Additionally, it uses the following new Theano functions and
    concepts: TODO

Motivation
++++++++++

Convolutional Neural Networks (CNN) are variants of MLPs which are inspired from
biology. From Hubel and Wiesel's early work on the cat's visual cortex [Hubel]_,
we know there exists a complex arrangement of cells within the visual cortex.
These cells are sensitive to small sub-regions of the input space, called a
**receptive field**, and are tiled in such a way as to cover the entire visual
field. These filters are local in input space and are thus better suited to
exploit the strong local correlation present in natural images.

Additionally, two basic cell types have been identified: simple cells (S) and
complex cells (C). Simple cells (S) respond maximally to specific edge-like
stimulus patterns within their receptive field. Complex cells (C) have larger
receptive fields and are locally invariant to the exact position of the
stimulus.

The visual cortex being the most powerful "vision" system in existence, it
seems natural to emulate its behavior. Many such neurally inspired models can be
found in the litterature. To name a few: the NeoCognitron [Fukushima]_, HMAX
[Serre]_ and LeNet-5 [LeCun]_. LeNet-5 will be the topic of this tutorial.


The Model
+++++++++

Sparse Connectivity
-------------------

CNNs exploit local correlation by enforcing a local connectivity pattern between
neurons of adjacent layers. The input hidden units in the i-th layer are
connected to a local subset of units in the (i-1)-th layer, which are spatially
contiguous. We can illustrate this graphically as follows:

TODO FIGURE

This architecture thus confines the learnt filters to be local.  Stacking many
such layers leads to filters which become increasingly "global" (i.e spanning a
larger region of pixel space) and abstract (as in any MLP).


Shared Weights
--------------

In CNNs, each sparse filter :math:`h_i` is additionally replicated across the
entire visual field. These "replicated" units form a **feature map**, which
share the same parametrization, i.e. the same weight vector and the same bias.
Replicating units in this way allows for features to be detected regardless of
their position in the visual field. Each feature map can thus be modeled as the
convolution of a single filter with the input image.

.. Note:: 
    Recall the following definition of convolution for a 1D signal.
    :math:`o[n] = f[n]*g[n] = \sum_{u=-\infty}^{\infty} f[u] g[u-n] = \sum_{u=-\infty}^{\infty} f[n-u] g[u]`.

    This can be extended to 2D as follows:
    :math:`o[m,n] = f[m,n]*g[m,n] = \sum_{u=-\infty}^{\infty} \sum_{v=-\infty}^{\infty} f[u,v] g[u-m,v-n]`.

In terms of notation, each feature map is denoted as :math:`h^K` , where
:math:`K` is the index of the feature map (**Important**: not to be confused
with the notation :math:`h^{(k)}`, referring to the k-th hidden layer). Each
feature map :math:`h^K` has a weight tensor :math:`W^K` and bias :math:`b_K`.

The input to the first hidden layer will typically be a 2D image. Convolving
this input with multiple 2D filters will create a set of feature maps
:math:`\{h^K, K=0..k\}`. These can be modeled as a 3D tensor: the leading
dimension indexes the feature map, while the two trailing dimensions refer to
the pixel position within each feature map.

Sadly, things get even more complicated at the second hidden layer ! This time
around, the input is already 3 dimensional. Generating the feature maps at the
second layer therefore requires 3D filters (one 2D filter for each input feature
map) ! The set of all filters required to compute the second layer is thus a 4D
tensor !!

The entries of the 4D tensor :math:`W` are denoted :math:`W^{KL}_{ij}`.
:math:`W^{KL}_{ij}` represents the weight connecting each pixel in the K-th
feature map at layer m, with pixel (i,j) in the L-th feature map of
layer (m-1).

Hopefully, the following figure will help clarify things.

.. figure:: images/cnn_explained.png
    :align: center

    **Figure 1**: The above figure shows two layers of a CNN, containing 4
    feature maps at layer (m-1) and 2 feature maps at layer m. The small
    rectangles at layer m indicate individual pixels with their receptive fields
    shown as colored rectangles. Note that the receptive fields spread across
    all 4 input feature maps. :math:`W^K` denotes the weights connecting a pixel
    with all units in its receptive fields.

Mathematically, the k-th feature map is determined as follows:

.. math::
    h^K_{ij} = \sigma ( (W^K * x)_{ij} + b_K ).


MaxPooling
----------

LeNet
-----

.. image:: images/mylenet.png
    :align: center


Going from MLP to convolutional MLP
+++++++++++++++++++++++++++++++++++


Putting it All Together
+++++++++++++++++++++++


.. Note:: TODO introduce API for sparse filters

References
+++++++++++++++++++++++

.. [Hubel] Hubel, D. and Wiesel, T. (1968). Receptive fields and functional architecture of monkey striate cortex. Journal of Physiology (London), 195, 215–243.

.. [Fukushima] Fukushima, K. (1980). Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics, 36, 193–202.

.. [Serre] Serre, T., Wolf, L., Bileschi, S., and Riesenhuber, M. (2007).  Robust object recog- nition with cortex-like mechanisms. IEEE Trans. Pattern Anal. Mach. Intell., 29(3), 411–426. Member-Poggio, Tomaso.

.. [LeCun] LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998d).  Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324.

.. rubric:: Footnotes

.. [#f1] For clarity, we use the word "unit" or "neuron" to refer to the
         artificial neuron and "cell" to refer to the biological neuron.


