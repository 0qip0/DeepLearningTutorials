Convolutional Neural Networks (LeNet)
=====================================

.. note::
    This section assumes the reader has already read through :doc:`logreg` and
    :doc:`mlp`. Additionally, it uses the following new Theano functions and
    concepts: TODO

Motivation
++++++++++

Convolutional Neural Networks (CNN) are variants of MLPs which are inspired from
biology. From Hubel and Wiesel's early work on the cat's visual cortex [Hubel]_,
we know there exists a complex arrangement of cells within the visual cortex.
These cells are sensitive to small sub-regions of the input space, called a
**receptive field**, and are tiled in such a way as to cover the entire visual
field. These filters are local in input space and are thus better suited to
exploit the strong local correlation present in natural images.

Additionally, two basic cell types have been identified: simple cells (S) and
complex cells (C). Simple cells (S) respond maximally to specific edge-like
stimulus patterns within their receptive field. Complex cells (C) have larger
receptive fields and are locally invariant to the exact position of the
stimulus.

The visual cortex being the most powerful "vision" system in existence, it
seems natural to emulate its behavior. Many such neurally inspired models can be
found in the litterature. To name a few: the NeoCognitron [Fukushima]_, HMAX
[Serre]_ and LeNet-5 [LeCun]_, which will be the focus of this tutorial.


Sparse Connectivity
+++++++++++++++++++

CNNs exploit local correlation by enforcing a local connectivity pattern between
neurons of adjacent layers. The input hidden units in the m-th layer are
connected to a local subset of units in the (m-1)-th layer, which are spatially
contiguous. We can illustrate this graphically as follows:

.. figure:: images/sparse_1D_nn.png
    :align: center

In the above, units have receptive fields of width 3 and are thus only
connected to 3 adjacent neurons in the layer below. The architecture thus
confines the learnt filters to be local. As shown above, stacking many such
layers leads to filters which become increasingly "global" however (i.e
spanning a larger region of pixel space). For example, the unit in hidden
layer (m+1) can encode a non-linear feature of width 5 (in terms of pixel
space).


Shared Weights
++++++++++++++

In CNNs, each sparse filter :math:`h_i` is additionally replicated across the
entire visual field. These "replicated" units form a **feature map**, which
share the same parametrization, i.e. the same weight vector and the same bias.

.. figure:: images/conv_1D_nn.png
    :align: center

In the above figure, we show 3 hidden units belonging to the same feature map.
Weights of the same color are shared, i.e. are constrained to be identical.
Gradient descent can still be used to learn such shared parameters, and
requires only a small change to the original algorithm. The gradient of a
shared weight is simply the sum of the gradients of the parameters being
shared.

Why are shared weights interesting ? Replicating units in this way allows for
features to be detected regardless of their position in the visual field.
Additionally, weight sharing offers a very efficient way to do this, since it
greatly reduces the number of free parameters to learn. By controlling model
capacity, CNNs tend to achieve better generalization on vision problems.


Details and Notation
++++++++++++++++++++

Conceptually, a feature map is obtained by convolving the input image with a
linear filter, adding a bias term and then applying a non-linear function. If
we denote the K-th feature map at a given layer as :math:`h^k`, whose filters
are determined by the weights :math:`W^k` and bias :math:`b_k`, then the
feature map :math:`h^k` is obtained as follows:

.. math::
    h^k_{ij} = \sigma ( (W^k * x)_{ij} + b_k ).

.. Note:: 
    Recall the following definition of convolution for a 1D signal.
    :math:`o[n] = f[n]*g[n] = \sum_{u=-\infty}^{\infty} f[u] g[u-n] = \sum_{u=-\infty}^{\infty} f[n-u] g[u]`.

    This can be extended to 2D as follows:
    :math:`o[m,n] = f[m,n]*g[m,n] = \sum_{u=-\infty}^{\infty} \sum_{v=-\infty}^{\infty} f[u,v] g[u-m,v-n]`.

To form a richer representation of the data, hidden layers are composed of
a set of multiple feature maps, :math:`\{h^{(k)}, k=0..K\}`.
The weights :math:`W` of this layer can be parametrized as a 4D tensor and
the biases :math:`b` as a vector. We illustrate this graphically as follows:

.. figure:: images/cnn_explained.png
    :align: center

    **Figure 1**: example of a convolutional layer

Here, we show two layers of a CNN, containing 4 feature maps at layer (m-1)
and 2 feature maps (:math:`h^0` and :math:`h^1`) at layer m. Pixels in
:math:`h^0` and :math:`h^1` (outlined as blue and red squares) are computed
from pixels of layer (m-1) which fall within their 2x2 receptive field (shown
as colored rectangles). Notice how the receptive field spans all four input
feature maps. The weights :math:`W^0` and :math:`W^1` of :math:`h^0` and
:math:`h^1` are thus 3D weight tensors. The leading dimension indexes the
input feature maps, while the other two refer to the pixel coordinates.

Putting it all together, :math:`W^{kl}_{ij}` denotes the weight connecting
each pixel of the k-th feature map at layer m, with the pixel at coordinates
(i,j) of the l-th feature map of layer (m-1). 


The ConvOp
++++++++++

ConvOp is the main workhorse for implementing a convolutional layer in Theano.
It is meant to replicate the behaviour of scipy.signal.convolve2d. Conceptually,
the ConvOp (once instantiated) takes two symbolic inputs:


* a 4D tensor corresponding to a mini-batch of input images. The shape of the
  tensor is as follows: [mini-batch size, number of input feature maps, image
  height, image width].

* a 4D tensor corresponding to the weight matrix :math:`W`. The shape of the
  tensor is: [number of feature maps at layer m, number of feature maps at
  layer m-1, image height, image width]


Below is the Theano code for implementing a convolutional layer similar to the
one of Figure 1. The input consists of 3 features maps (a color image) of size
120x160. We use two convolutional filters with 9x9 receptive fields.

.. code-block:: python

        from theano.sandbox import conv
        rng = numpy.random.RandomState(23455)

        # create type corresponding to 4D tensor
        dtensor4D = T.TensorType('float64', [False]*4)

        # instantiate 4D tensor for input
        input = dtensor4D(name='input')

        # initialize shared variable for weights.
        w_shp = (2, 3, 9, 9)
        w_bound =  numpy.sqrt(3 * 9 * 9)
        w = theano.shared( numpy.asarray(
                    rng.uniform(
                        low=-1.0 / w_bound,
                        high=1.0 / w_bound,
                        size=w_shp),
                    dtype=input.dtype))
        
        # initialize shared variable for bias (1D tensor)
        b_shp = (2,)
        b = theano.shared( numpy.asarray(
                    rng.uniform(low=-.0, high=0., size=(2,)),
                    dtype=input.dtype))

        # compute convolution of input with filters
        conv_out = conv.conv2d(input, w)

        # add bias and apply activation function
        output = T.nnet.sigmoid(conv_out + b.dimshuffle('x', 0, 'x', 'x'))

        # create theano function to compute filtered images
        f = theano.function([input], [output])


Lets have a little bit of fun with this...

.. code-block:: python

        import pylab
        from PIL import Image

        # open random image of dimensions 639x516
        img = Image.open(open('images/3wolfmoon.jpg'))
        img = numpy.asarray(img, dtype='float64')/256.

        # put image in 4D tensor of shape (1,3,height,width)
        img_ = img.swapaxes(0,2).swapaxes(1,2).reshape(1,3,639,516)
        filtered_img = f(img_)[0]

        # plot original image and first and second components of output
        pylab.subplot(1,3,1); pylab.axis('off'); pylab.imshow(img)
        pylab.gray();
        pylab.subplot(1,3,2); pylab.axis('off'); pylab.imshow(filtered_img[0,0,:,:])
        pylab.subplot(1,3,3); pylab.axis('off'); pylab.imshow(filtered_img[0,1,:,:])
        pylab.show()


This should generate the following output.

.. image:: images/3wolfmoon_output.png
    :align: center

Notice that a randomly initialized filter acts very much like an edge detector
! 

Also of note, note that we use the same weight initialization formula as
with the MLP. Weights are sampled randomly from a uniform distribution in the
range [-1/fan-in, 1/fan-in], where fan-in is the number of inputs to a hidden
unit. For MLPs, this was the number of units in the layer below. For CNNs
however, we have to take into account the number of input feature maps and the
size of the receptive fields.


MaxPooling
----------

LeNet
-----

.. image:: images/mylenet.png
    :align: center


Going from MLP to convolutional MLP
+++++++++++++++++++++++++++++++++++


Putting it All Together
+++++++++++++++++++++++


.. Note:: TODO introduce API for sparse filters

References
+++++++++++++++++++++++

.. [Hubel] Hubel, D. and Wiesel, T. (1968). Receptive fields and functional architecture of monkey striate cortex. Journal of Physiology (London), 195, 215–243.

.. [Fukushima] Fukushima, K. (1980). Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics, 36, 193–202.

.. [Serre] Serre, T., Wolf, L., Bileschi, S., and Riesenhuber, M. (2007).  Robust object recog- nition with cortex-like mechanisms. IEEE Trans. Pattern Anal. Mach. Intell., 29(3), 411–426. Member-Poggio, Tomaso.

.. [LeCun] LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998d).  Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324.

.. rubric:: Footnotes

.. [#f1] For clarity, we use the word "unit" or "neuron" to refer to the
         artificial neuron and "cell" to refer to the biological neuron.


