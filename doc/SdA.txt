Stacked Denoising Autoencoders (SdA)
====================================

.. note::
  This section assumes the reader has already read through :doc:`logreg`
  and :doc:`mlp`. Additionally it uses the following Theano functions
  and concepts : TODO


The Stacked Denoising Autoencoder (SdA) is an extension of the stacked 
autoencoder and it was introduced in [Vincent08]_. We will start the 
tutorial with a short digression on :ref:`autoencoders`
and then move on to how classical
autoencoders are extended to denoising autoencoders (:ref:`dA`).
Throughout the following subchapters we will stick as close as possible to 
the original paper ( [Vincent08]_ ).


.. _autoencoders:

Autoencoders
+++++++++++++

An autoencoder takes an input :math:`\mathbf{x} \in [0,1]^d` and first 
maps it to a hidden representation :math:`\mathbf{y} \in [0,1]^{d'}` 
through a deterministic mapping:

.. math::
  
  \mathbf{y} = s(\mathbf{W}\mathbf{x} + \mathbf{b})

The latent representation :math:`\mathbf{y}` is then mapped back into a
"reconstructed" vector :math:`\mathbf{z}` of same shape as
:math:`\mathbf{x}` through a similar transformation, namely:

.. math::

  \mathbf{z} = s(\mathbf{W'}\mathbf{y} + \mathbf{b'})

The weights matrix :math:`\mathbf{W'}` of the reverse mapping may be
optionally constrained by :math:`\mathbf{W'} = \mathbf{W}`, this is 
called using `tied weights`. The parameters of this model (nameley 
:math:`\mathbf{W}`, :math:`\mathbf{b}`, 
:math:`\mathbf{b'}` and, if one doesn't use tied weights, also 
:math:`\mathbf{W'}`) are optimized such that the average reconstruction 
error is minimized. The reconstruction error can be measured using the 
traditional `squared error` :math:`L(\mathbf{x}, \mathbf{z}) = | \mathbf{x} - \mathbf{z} |^2`, 
or if the input is interpreted as either bit vectors or vectors of 
bit probabilities by the reconstruction `cross-entropy` defined as : 

.. math::

  L_{H} (\mathbf{x}, \mathbf{z} = - \sum^d_{k=1}[\mathbf{x}_k \log
          \mathbf{z}_k + (1 - \mathbf{x}_k)\log(1 - \mathbf{z}_k)] 

We want to implent this behaviour using Theano, in the form of a class,
that could be afterwards used in constructing a stacked autoencoder.

.. code-block:: python

  class AutoEncoder(object):

    def __init__(self, n_visible= 784, n_hidden= 500, input = None):

Note that we pass the ``input``  to the autoencoder as a
parameter. This is such that later we can concatenate layers of 
autoencoders to form a deep network.

Next step is to create shared variables for the parameters of the 
autoencoder ( :math:`\mathbf{W}`, :math:`\mathbf{b}` and 
:math:`\mathbf{b'}`, since we are using tied weights in this tutorial ): 

.. code-block:: python

    # initial values for weights and biases
    # note : W' was written as `W_prime` and b' as `b_prime`

    # W is initialized with `initial_W` which is uniformely sampled
    # from -6./sqrt(n_visible+n_hidden) and 6./sqrt(n_hidden+n_visible)
    # the output of uniform if converted using asarray to dtype 
    # theano.config.floatX so that the code is runable on GPU
    initial_W = numpy.asarray( numpy.random.uniform( \
              low = -numpy.sqrt(6./(n_visible+n_hidden)), \
              high = numpy.sqrt(6./(n_visible+n_hidden)), \
              size = (n_visible, n_hidden)), dtype = theano.config.floatX)
    initial_b       = numpy.zeros(n_hidden)
    initial_b_prime= numpy.zeros(n_visible)
     
    
    # theano shared variables for weights and biases
    self.W       = theano.shared(value = initial_W,       name = "W")
    self.b       = theano.shared(value = initial_b,       name = "b")
    # tied weights, therefore W_prime is W transpose
    self.W_prime = W.T 
    self.b_prime = theano.shared(value = initial_b_prime, name = "b'")

Now we can compute the latent representation and the reconstructed
signal : 

.. code-block:: python

    self.y    = T.nnet.sigmoid(T.dot(x,      self.W      ) + self.b)
    z         = T.nnet.sigmoid(T.dot(self.y, self.W_prime) + self.b_prime)
    self.L    = - T.sum( x*T.log(z) + (1-x)*T.log(1-z), axis=1 ) 
    self.cost = T.mean(self.L)

Training the autoencoder consist now in updating the parameters ``W``, 
``b`` and ``b_prime`` by stochastic gradient descent such that the 
cost is minimized.

.. code-block:: python

  train = theano.function( [x], cost, updates = { \
            self.W       : self.W       - T.grad(self.cost, self.W      )*learning_rate, 
            self.b       : self.b       - T.grad(self.cost, self.b      )*learning_rate,
            self.b_prime : self.b_prime - T.grad(self.cost, self.b_prime)*learning_rate})

Note that for the stacked denoising autoencoder we will not use the
``train`` function as defined here. 


Denoising Autoencoders (dA)
+++++++++++++++++++++++++++

The idea behind of denoising autoencoders is simple. In order to enforce
the hidden layer to discover more roboust features, we first corrupt the
input, then give it to the autoencoder and train it
such that it tries tp reconstruct the uncorrupted version of the data. 
There are different perspectives that can help understand why this process
help in training ( manifold learning perspective, stochastic operator 
perspective, bottom-up -- information theoretic perspective, top-down --
generative model perspective ) all of them being explained in [Vincent08]_.


To convert the autoencoder class into a denoising autoencoder class all we 
need to do is to add a corrupting step. The type of corruption used here is 
to mask randomly entries of the input by making them zero. The code below 
does just that : 

.. code-block:: python

	from theano.tensor,shared_randomstreals import RandomStreams

	theano_rng = RandomStreams()
	corrupted_x = x * theano.rng.binomial(x.shape, 1, 0.9)

The final denoising autoencoder class becomes : 

.. code-block:: python

   class dA(object):

       def __init__(self, n_visible= 784, n_hidden= 500, input= None):

    	  self.n_visible = n_visible
          self.n_hidden  = n_hidden
    
          # create a Theano random generator that gives symbolic random values
          theano_rng = RandomStreams()
          # create a numpy random generator
          numpy_rng = numpy.random.RandomState()
    
     
          # initial values for weights and biases
          # note : W' was written as `W_prime` and b' as `b_prime`

    	  # W is initialized with `initial_W` which is uniformely sampled
    	  # from -6./sqrt(n_visible+n_hidden) and 6./sqrt(n_hidden+n_visible)
          # the output of uniform if converted using asarray to dtype 
    	  # theano.config.floatX so that the code is runable on GPU
    	  initial_W = numpy.asarray( numpy.random.uniform( \
              low = -numpy.sqrt(6./(n_visible+n_hidden)), \
              high = numpy.sqrt(6./(n_visible+n_hidden)), \
              size = (n_visible, n_hidden)), dtype = theano.config.floatX)
    	  initial_b       = numpy.zeros(n_hidden)
    	  initial_b_prime= numpy.zeros(n_visible)
     
    
    	  # theano shared variables for weights and biases
    	  self.W       = theano.shared(value = initial_W,       name = "W")
          self.b       = theano.shared(value = initial_b,       name = "b")
    	  # tied weights, therefore W_prime is W transpose
    self.W_prime = self.W.T 
    self.b_prime = theano.shared(value = initial_b_prime, name = "b'")

    # if no input is given, generate a variable representing the input
    if input == None : 
        # we use a matrix because we expect a minibatch of several examples,
        # each example being a row
        x = T.dmatrix(name = 'input') 
    else:
        x = input
    # Equation (1)
    # note : first argument of theano.rng.binomial is the shape(size) of 
    #        random numbers that it should produce
    #        second argument is the number of trials 
    #        third argument is the probability of success of any trial
    #
    #        this will produce an array of 0s and 1s where 1 has a 
    #        probability of 0.9 and 0 if 0.1
    tilde_x  = theano_rng.binomial( x.shape,  1,  0.9) * x
    # Equation (2)
    # note  : y is stored as an attribute of the class so that it can be 
    #         used later when stacking dAs. 
    self.y   = T.nnet.sigmoid(T.dot(tilde_x, self.W      ) + self.b)
    # Equation (3)
    z        = T.nnet.sigmoid(T.dot(self.y, self.W_prime) + self.b_prime)
    # Equation (4)
    self.L = - T.sum( x*T.log(z) + (1-x)*T.log(1-z), axis=1 ) 
    # note : L is now a vector, where each element is the cross-entropy cost 
    #        of the reconstruction of the corresponding example of the 
    #        minibatch. We need to compute the average of all these to get 
    #        the cost of the minibatch
    self.cost = T.mean(self.L)
    # note : y is computed from the corrupted `tilde_x`. Later on, 
    #        we will need the hidden layer obtained from the uncorrupted 
    #        input when for example we will pass this as input to the layer 
    #        above
    self.hidden_values = T.nnet.sigmoid( T.dot(x, self.W) + self.b)



.. _stacked_autoencoders:

Stacked Autoencoders
++++++++++++++++++++

Autoencoders can be stacked to form a deep network ([Bengio07]_ ) by feeding 
the latent representation of the autoencoder found on the layer below as 
input to the current layer. The "pre-training" of such an architecture
is done one layer at a time. Once the first :math:`k` layers are trained,
we can train the :math:`k+1`-th layer because we can now compute the 
"correct" latent representation generated by the layer below. Once all 
layers are trained, we add a logistic regression on top of the last
layer and treat the entire network as a multilayer perceptron.  This 
stage of training is called fine-tunning.

This can be easily implemented in Theano, using the class defined
before. We start off by creating a class that defines a stacked
autoencoder.

.. code-block:: python

  class StackedAutoencoder():
   
    def __init__(self, input, n_ins, hidden_layers_sizes, n_outs):
        """ This class is made to support a variable number of layers. 

        :param input: symbolic variable describing the input of the SdA

        :param n_ins: dimension of the input to the sdA

        :param n_layers_sizes: intermidiate layers size, must contain 
        at least one value

        :param n_outs: dimension of the output of the network
        """

Next step, we create an autoencoder for each layer and link them
together:

.. code-block:: python

        self.layers =[]

        if len(hidden_layers_sizes) < 1 :
            raiseException (' You must have at least one hidden layer ')

        # add first layer:
        layer = AutoEncoder(n_ins, hidden_layers_sizes[0], input = input)
        self.layers += [layer]
        # add all intermidiate layers
        for i in xrange( 1, len(hidden_layers_sizes) ):
            # input size is that of the previous layer
            # input is the output of the last layer inserted in our list 
            # of layers `self.layers`
            layer = AutoEncoder( hidden_layers_sizes[i-1],             \
                                 hidden_layers_sizes[i],               \
                                 input = self.layers[-1].hidden_values )
            self.layers += [layer]
        

        self.n_layers = len(self.layers)


Note that during the second stage of training (fine-tuning) we need to 
use the weights of the autoencoders to define a multilayer perceptron.
This is already given by the above lines of code, in the sense that 
the ``hidden_values`` of the last autoencoder already computes what
should be the input of the logistic regression layer that sits at the 
top of the MLP. All we need now is to add the logistic layer. We will 
use the ``LogisticRegression`` class which is identical with the one 
implemented in :ref:`logreg`. 

.. code-block:: python

        # add a logistic layer on top
        self.logLayer = LogisticRegression(\
                         input = self.layers[-1].hidden_values,\
                         n_in = hidden_layers_sizes[-1], n_out = n_outs)






Putting it all together
+++++++++++++++++++++++

TODO


Running the Code
++++++++++++++++

TODO

Tips and Tricks
+++++++++++++++

TODO

References
++++++++++

.. [Vincent08] Vincent, P., Larochelle H., Bengio Y. and Manzagol P.A.
     (2008). Extracting and Composing Robust Features with Denoising
     Autoencoders. ICML'08, pp. 1096 - 1103

.. [Bengio07] Bengio Y., Lamblin P., Popovici D. and Larochelle H.
     (2007). Greedy Layer-Wise Training of Deep Networks. NIPS'06, pp
     153-160

