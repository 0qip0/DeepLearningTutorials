Classifying MNIST digits using Logistic Regression
==================================================

.. note::
    This sections assumes the reader is familiar with the following Theano
    concepts: shared variables, basic arithmetic ops, tensor.grad.
    ((TODO: html links into theano documentation))


In this section, we show how Theano can be used to implement the most basic
classifier: the logistic regression. We start off with a quick primer of the
model, which serves as a refresher but also to anchor the notation and show
how mathematical expressions can easily be mapped onto theano graphs.

In the deepest of machine learning traditions, this tutorial will tackle the exciting
problem of MNIST digit classification.

The Model
+++++++++

Logistic regression is a probabilistic, linear classifier. It is parametrized
by a weight matrix :math:`W` and a bias vector :math:`b`. Classification is
done by projecting data points onto a set of hyperplanes, the distance to
which is used to determine a class membership probability. 

Mathematically, this can be written as:

.. math::
  P(Y=i|x, W,b) &= softmax_i(W x + b) \\
                &= \frac {e^{W_i x + b_i}} {\sum_j e^{W_j x + b_j}}

The output of the model or prediction is then done by taking the argmax of the vector whose i'th element is P(Y=i|x).

.. math::
  y_{pred} = argmax_i P(Y=i|x,W,b)



The code to do this in theano is the following:

.. code-block:: python

  # generate symbolic variables for input (x and y represent a
  # minibatch)
  x = T.fmatrix()
  y = T.lvector()

  # allocate shared variables model params
  b = theano.shared(numpy.random(10))
  W = theano.shared(numpy.random(784,10))

  # symbolic expression for computing the vector of 
  # class-membership probabilities 
  p_y_given_x = T.softmax(T.dot(x,w)+b)

  # compiled theano function that returns the vector of class-membership
  # probabilities
  get_p_y_given_x = theano.function( x, p_y_given_x)

  # print the probability of some example represented by x_value 
  # x_value is not a symbolic variable but a numpy array describing the
  # datapoint
  print 'Probability that x is of class %i is %f' % i, get_p_y_given_x(x_value)[i]

  # symbolic description of how to compute prediction as class whose probability 
  # is maximal 
  y_pred = T.argmax(p_y_given_x)

  # compiled theano function that returns this value
  classify = theano.function([x,y], y_pred)


We first start by allocating symbolic variables for the inputs
:math:`x,y`. Afterwards we allocate shared variables for the parameters :math:`W,b`.
This step declares them both as symbolic theano
variables, but also initializes their contents. The dot and softmax operators
are then used to compute the vector :math:`P(Y|x, W,b)`. The resulting
variable p_y_given_x is a symbolic variable pointing to a vector. The function 
`get_p_y_given_x` computs this vector for a given x. The output of the
function is a vector and can thus be index to retrieve a
particular entry :math:`P(Y=i|x, W,b)`. The final model prediction is then
computed using the T.argmax operator.

Up to this point, we have only defined the graph of calculations which theano
should perform. To calculate the actual prediction, we construct a function
``classify``, which takes as input x and y and outputs the vector of
predicted classes for each example in x.

At this point in time, the model we have defined does not do anything useful
because the parameters are random.  The following section will thus cover how
to learn the optimal parameters.


.. note::
    For a complete list of Theano ops, see: TODO


Defining a Loss Function
++++++++++++++++++++++++

Learning optimal model parameters involves minimizing a loss function. In the
case of multi-class logistic regression, it is very common to use the negative
likelihood as the loss. This is equivalent to maximizing the likelihood of the
data set :math:`\cal{D}` under the model parameterized by :math:`\theta`. Let
us first start by defining the likelihood :math:`\cal{L}` and loss
:math:`\ell`:

.. math::

   \mathcal{L} (\theta=\{W,b\}, \mathcal{D}) = 
     \sum_{i=0}^{|\mathcal{D}|} log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\
   \ell (\theta=\{W,b\}, \mathcal{D}) = - \mathcal{L} (\theta=\{W,b\}, \mathcal{D})

While entire books are dedicated to the topic of minimization, gradient
descent is by far the simplest method for minimizing arbitrary non-linear
functions. This tutorial will use the method of stochastic gradient method with
mini-batches (MSGD, see :doc:`optimization`).

The following Theano code defines the loss for a given minibatch:

.. code-block:: python

  loss = T.sum(T.log(p_y_given_x)[y])

.. note::
    In practice, we will use the mean (T.mean) instead of the sum. This
    allows for the learning rate to be independent of the minibatch size.
    

Creating a LogisticRegression class
+++++++++++++++++++++++++++++++++++

We now have all the tools we need to define the following class, which
encapsulates the basic behaviour for LogisticRegression:

.. code-block:: python

    class LogisticRegression(object):


        def __init__(self, input, n_in, n_out):
            """ Initialize the parameters of the logistic regression
            :param input: symbolic variable that describes the input of the 
            architecture ( one minibatch)
            :param n_in: number of input units, the dimension of the space in 
            which the datapoint lies
            :param n_out: number of output units, the dimension of the space in 
            which the target lies
            """ 

            # initialize with 0 the weights W as a matrix of shape (n_in, n_out) 
            self.W = shared( value=numpy.zeros((n_in,n_out),
                                                dtype = theano.config.floatX) )
            # initialize the baises b as a vector of n_out 0s
            self.b = shared( value=numpy.zeros((n_out,), 
                                                dtype = theano.config.floatX) )

            # compute vector of class-membership probabilities in symbolic form
            self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W)+self.b)

            # compute prediction as class whose probability is maximal in 
            # symbolic form
            self.y_pred=T.argmax(self.p_y_given_x, axis=1)


        def negative_log_likelihood(self, y):
            """Return the negative log-likelihood of the prediction of this model
            under a given target distribution.  
            :param y: corresponds to a vector that gives for each example the
            :correct label
            """
            # TODO: inline NLL formula, refer to theano function
            return -theano.log(self.p_y_given_x)[y]


We instantiate the class and declare a global cost which we wish to minimize:

.. code-block:: python

    # allocate symbolic variables for the data
    x = T.fmatrix()  # the data is presented as rasterized images
    y = T.lvector()  # the labels are presented as 1D vector of [long int] labels

    # construct the logistic regression class
    classifier = LogisticRegression( \
                   input=x.reshape((batch_size,28*28)), n_in=28*28, n_out=10)

    # the cost we minimize during training is the negative log likelihood of 
    # the model in symbolic format
    cost = classifier.negative_log_likelihood(y).mean() 


Learning the Model
++++++++++++++++++

To implement MSGD in most programming languages (C/C++, Matlab, Python), one
would start by manually deriving the expressions for the gradient of the loss
with respect to the parameters: in this case :math:`\partial{\ell}/\partial{W}`,
and :math:`\partial{\ell}/\partial{b}`, This can get pretty tricky for complex
models, as expressions for :math:`\partial{\ell}/\partial{\theta}` can get
fairly complex, especially when taking into account problems of numerical
stability.  With Theano, this work is greatly simplified as it performs
automatic differentiation and applies certain math transforms to improve
numerical stability.


.. code-block:: python

    # set a learning rate
    learning_rate=0.01

    # compute the gradient of cost with respect to theta = (W,b)
    g_W = T.grad(cost, classifier.W)
    g_b = T.grad(cost, classifier.b)

    # specify how to update the parameters of the model as a dictionary
    updates ={classifier.W: classifier.W - numpy.asarray(learning_rate)*g_W,\
              classifier.b: classifier.b - numpy.asarray(learning_rate)*g_b}

    # compiling a theano function `train_model` that returns the cost, but in
    # the same time updates the parameter of the model based on the rules
    # defined in `updates`
    train_model = pfunc([x, y], cost, updates = updates )


In the above code, ``T.grad`` is used to define the symbolic gradients of the
cost functions with respect to :math:`W,b`. We then define an ``updates``
dictionary, which contains, for each parameter, the stochastic gradient update
operation. The function ``train_model`` is then defined such that:

* the inputs are the mini-batch :math:`x` with corresponding labels :math:`y`
* the return value is the cost/loss associated with inputs (x,y)
* on every function call, it applies the operations defined in the ``updates``
  dictionary. 

Each time ``train_model(x,y)`` function is called, it will thus compute and return the appropriate cost, while also performing a step of MSGD. The entire learning algorithm thus consists in looping over all examples in the dataset, and repeatedly calling the ``train_model`` function.

The finished product is as follows:

.. todo:
   talk about test_model function

.. code-block:: python

    # early-stopping parameters
    patience              = 5000  # look as this many examples regardless
    patience_increase     = 2     # wait this much longer when a new best is 
                                  # found
    improvement_threshold = 0.995 # a relative improvement of this much is 
                                  # considered significant
    validation_frequency  = 1000  # make this many SGD updates between 
                                  # validations

    best_params          = None
    best_validation_loss = float('inf')
    test_score           = 0.

    # have a maximum of `n_iter` iterations through the entire dataset
    for iter in xrange(n_iter* len(train_batches)):

        # get epoch and minibatch index
        epoch           = iter / len(train_batches)
        minibatch_index =  iter % len(train_batches)

        # get the minibatches corresponding to `iter` modulo
        # `len(train_batches)`
        x,y = train_batches[ minibatch_index ]
        cost_ij = train_model(x,y)

        if (iter+1) % validation_frequency == 0: 
            # compute zero-one loss on validation set 
            this_validation_loss = 0.
            for x,y in valid_batches:
                # sum up the errors for each minibatch
                this_validation_loss += test_model(x,y)
            # get the average by dividing with the number of minibatches
            this_validation_loss /= len(valid_batches)

            print('epoch %i, validation error %f' % 
                                (epoch, this_validation_loss))

            #improve patience 
            if this_validation_loss < best_validation_loss *  \
                                      improvement_threshold :
                patience = max(patience, iter * patience_increase)


            # if we got the best validation score until now
            if this_validation_loss < best_validation_loss:
                best_validation_loss = this_validation_loss
                # test it on the test set
            
                test_score = 0.
                for x,y in test_batches:
                    test_score += test_model(x,y)
                test_score /= len(test_batches)
                print('     epoch %i, test error of best model %f' % 
                                    (epoch, test_score))

        if patience <= iter :
                break


    print(('Optimization complete with best validation score of %f,'
           'with test performance %f') %  (best_validation_loss, test_score))




.. rubric:: Footnotes

.. [#f1] For smaller datasets and simpler models, more sophisticated descent
         algorithms can be more effective. The sample code for logistic regression
         demonstrates how to use SciPy's conjugate gradient solver with theano.


