.. _DBN:

Deep Belief Networks
====================

.. note::
  This section assumes the reader has already read through :doc:`logreg`
  and :doc:`mlp` and :doc:`rbm`. Additionally it uses the following Theano
  functions and concepts : `T.tanh`_, `shared variables`_, `basic arithmetic
  ops`_, `T.grad`_, `Random numbers`_, `floatX`_. If you intend to run the
  code on GPU also read `GPU`_.

.. _T.tanh: http://deeplearning.net/software/theano/tutorial/examples.html?highlight=tanh

.. _shared variables: http://deeplearning.net/software/theano/tutorial/examples.html#using-shared-variables

.. _basic arithmetic ops: http://deeplearning.net/software/theano/tutorial/adding.html#adding-two-scalars

.. _T.grad: http://deeplearning.net/software/theano/tutorial/examples.html#computing-gradients

.. _floatX: http://deeplearning.net/software/theano/library/config.html#config.floatX

.. _GPU: http://deeplearning.net/software/theano/tutorial/using_gpu.html 

.. _Random numbers: http://deeplearning.net/software/theano/tutorial/examples.html#using-random-numbers


.. note::
    The code for this section is available for download `here`_.

.. _here: http://deeplearning.net/tutorial/code/DBN.py


Deep Belief Networks
++++++++++++++++++++

A Deep Belief Network [Hinton06]_ with :math:`\ell` layers models the joint
distribution between observed vector :math:`x` and :math:`\ell` hidden layers :math:`h^k` as
follows:

.. math::
    :label: dbn
 
    P(x, h^1, \ldots, h^{\ell}) = \left(\prod_{k=0}^{\ell-2} P(h^k|h^{k+1})\right) P(h^{\ell-1},h^{\ell})

where :math:`x=h^0`, :math:`P(h^{k-1} | h^k)` is a conditional distribution for
visible hidden units in an RBM associated with level :math:`k` of the DBN,
and :math:`P(h^{\ell-1}, h^{\ell})` is the visible-hidden joint distribution
in the top-level RBM. This is illustrated in the figure below.


.. figure:: images/DBN3.png
    :align: center


In practice, such a model is trained in two stages, a pretraining stage and 
a fine-tunning one. During pretraining, you go through the layers starting 
from the bottom to top and train each layer seperately. At this point you 
can see your model as a set of disconnected RBMs that have to be trained. 
To train the RBM corresponding to layer :math:`k` though, you need to have
the input of this layer (which depends on the RBM corresponding to the first
:math:`k-1` layers) and this is why you have to go through the RBMs in a 
specific order. A trick that you can do (and would actually improve the 
time run of your code, given that you have sufficient memory available), 
is to compute how the network, up to layer :math:`k-1`, transforms your 
data. Namely, you start by training your first layer RBM. Once it 
is trained, you can compute the hidden units values for every datapoint in 
your dataset and store this as a new dataset that you will use to train the 
RBM corresponding to layer 2. Once you trained the RBM for layer 2, you 
compute, in a similar fashion, the dataset for layer 3 and so on. You 
can see now, that at this point, the RBMs are trained individually, and 
they just provide (one to the other) a non-linear transformation of the 
input.  Once all RBMs are trained, you can start fine-tunning the model.

During fine-tunning, you drop the RBMs and just use the learned weights 
and biases to create a MLP. Layer :math:`k` of the MLP will have the 
weights and biases of the RBM corresponding to layer :math:`k`. On top 
of these layers you add a logistic regression layer and train the model
using (stochastic) gradient descent. Note, that for classification you 
use the RBMs just to initialize your MLP, which you will use in the end 
as your model. 

To implement this in Theano we will use the class defined before for the
RBM tutorial. As an observation, the code for the DBN is very similar 
with the one for SdA, mostly the difference being that we use the RBM class
instead of the dA class.

We start off, by defining the DBN class which will store the layers of the 
MLP together with the RBMs that are linked to them.

.. code-block:: python

    class DBN(object):

        def __init__(self, numpy_rng, theano_rng = None, n_ins = 784,
                    hidden_layers_sizes=[500,500], n_outs = 10):
            """This class is made to support a variable number of layers. 

            :type numpy_rng: numpy.random.RandomState
            :param numpy_rng: numpy random number generator used to draw initial 
                    weights

            :type theano_rng: theano.tensor.shared_randomstreams.RandomStreams
            :param theano_rng: Theano random generator; if None is given one is 
                           generated based on a seed drawn from `rng`

            :type n_ins: int
            :param n_ins: dimension of the input to the DBN

            :type n_layers_sizes: list of ints
            :param n_layers_sizes: intermidiate layers size, must contain 
                               at least one value

            :type n_outs: int
            :param n_outs: dimension of the output of the network
            """

            self.sigmoid_layers = []
            self.rbm_layers     = []
            self.params         = []
            self.n_layers       = len(hidden_layers_sizes)

            assert self.n_layers > 0

            if not theano_rng:
                theano_rng = RandomStreams(numpy_rng.randint(2**30))

            # allocate symbolic variables for the data
            self.x  = T.matrix('x')  # the data is presented as rasterized images
            self.y  = T.ivector('y') # the labels are presented as 1D vector of 
                                     # [int] labels

``self.sigmoid_layers`` will store the sigmoid layers of the MLP facade, while
``self.rbm_layers`` will store  the RBMs associated with the layers of the MLP. 

Next step, we construct ``n_layers`` sigmoid layers (we use the
``SigmoidalLayer`` class introduced in :ref:`mlp`, with the only
modification that we replaced the non-linearity from ``tanh`` to the
logistic function :math:`s(x) = \frac{1}{1+e^{-x}}`) and ``n_layers``
denoising autoencoders, where ``n_layers`` is the depth of our model.
We link the sigmoid layers such that they form an MLP, and construct
each RBM such that they share the weight matrix and the 
bias of the encoding part with its corresponding sigmoid layer.


.. code-block:: python

        for i in xrange( self.n_layers ):
            # construct the sigmoidal layer

            # the size of the input is either the number of hidden units of the layer below or
            # the input size if we are on the first layer
            if i == 0 :
                input_size = n_ins
            else:
                input_size = hidden_layers_sizes[i-1]

            # the input to this layer is either the activation of the hidden layer below or the
            # input of the DBN if you are on the first layer
            if i == 0 : 
                layer_input = self.x
            else:
                layer_input = self.sigmoid_layers[-1].output

            sigmoid_layer = HiddenLayer(rng   = numpy_rng, 
                                           input = layer_input, 
                                           n_in  = input_size, 
                                           n_out = hidden_layers_sizes[i],
                                           activation = T.nnet.sigmoid)
            
            # add the layer to our list of layers 
            self.sigmoid_layers.append(sigmoid_layer)

            # its arguably a philosophical question...  but we are going to only declare that
            # the parameters of the sigmoid_layers are parameters of the DBN. The visible
            # biases in the RBM are parameters of those RBMs, but not of the DBN.
            self.params.extend(sigmoid_layer.params)
        
            # Construct an RBM that shared weights with this layer
            rbm_layer = RBM(numpy_rng = numpy_rng, theano_rng = theano_rng, 
                          input = layer_input, 
                          n_visible = input_size, 
                          n_hidden  = hidden_layers_sizes[i],  
                          W = sigmoid_layer.W, 
                          hbias = sigmoid_layer.b)
            self.rbm_layers.append(rbm_layer)        


All we need now is to add the logistic layer on top of the sigmoid
layers such that we have an MLP. We will 
use the ``LogisticRegression`` class introduced in :ref:`logreg`. 

.. code-block:: python

        # We now need to add a logistic layer on top of the MLP
        self.logLayer = LogisticRegression(\
                         input = self.sigmoid_layers[-1].output,\
                         n_in = hidden_layers_sizes[-1], n_out = n_outs)
        self.params.extend(self.logLayer.params)

        # construct a function that implements one step of fine-tuning compute the cost for
        # second phase of training, defined as the negative log likelihood 
        self.finetune_cost = self.logLayer.negative_log_likelihood(self.y)

        # compute the gradients with respect to the model parameters
        # symbolic variable that points to the number of errors made on the
        # minibatch given by self.x and self.y
        self.errors = self.logLayer.errors(self.y)

The class also provides a method that generates training functions for
each of the RBM associated with the different layers. 
They are returned as a list, where element :math:`i` is a function that
implements one step of training the ``RBM`` correspoinding to layer 
:math:`i`.


.. code-block:: python

    def pretraining_functions(self, train_set_x, batch_size):
        ''' Generates a list of functions, for performing one step of gradient descent at a
        given layer. The function will require as input the minibatch index, and to train an
        RBM you just need to iterate, calling the corresponding function on all minibatch
        indexes.

        :type train_set_x: theano.tensor.TensorType
        :param train_set_x: Shared var. that contains all datapoints used for training the RBM
        :type batch_size: int
        :param batch_size: size of a [mini]batch
        '''

        # index to a [mini]batch
        index            = T.lscalar('index')   # index to a minibatch

In order to be able to change the learning rate
during training we associate a Theano variable to it that has a 
default value.

.. code-block:: python

        learning_rate    = T.scalar('lr')    # learning rate to use

        # number of batches
        n_batches = train_set_x.value.shape[0] / batch_size
        # begining of a batch, given `index`
        batch_begin = index * batch_size
        # ending of a batch given `index`
        batch_end = batch_begin+batch_size

        pretrain_fns = []
        for rbm in self.rbm_layers:

            # get the cost and the updates list
            # TODO: change cost function to reconstruction error
            cost,updates = rbm.cd(learning_rate, persistent=None)

            # compile the theano function    
            fn = theano.function(inputs = [index, 
                              theano.Param(learning_rate, default = 0.1)], 
                    outputs = cost, 
                    updates = updates,
                    givens  = {self.x :train_set_x[batch_begin:batch_end]})
            # append `fn` to the list of functions
            pretrain_fns.append(fn)

        return pretrain_fns

Now any function ``pretrain_fns[i]`` takes as arguments ``index`` and 
optionally ``lr`` -- the
learning rate. Note that the name of the parameters are the name given 
to the Theano variables when they are constructed, not the name of the 
python variables (``learning_rate``). Keep this 
in mind when working with Theano. 

In the same fashion we build a method for constructing function required 
during finetuning ( a ``train_model``, a ``validate_model`` and a
``test_model`` funcion). 

.. code-block:: python


    def build_finetune_functions(self, datasets, batch_size, learning_rate):
        '''Generates a function `train` that implements one step of finetuning, a function
        `validate` that computes the error on a batch from the validation set, and a function
        `test` that computes the error on a batch from the testing set

        :type datasets: list of pairs of theano.tensor.TensorType
        :param datasets: It is a list that contain all the datasets;  the has to contain three
        pairs, `train`, `valid`, `test` in this order, where each pair is formed of two Theano
        variables, one for the datapoints, the other for the labels
        :type batch_size: int
        :param batch_size: size of a minibatch
        :type learning_rate: float
        :param learning_rate: learning rate used during finetune stage
        '''

        (train_set_x, train_set_y) = datasets[0]
        (valid_set_x, valid_set_y) = datasets[1]
        (test_set_x , test_set_y ) = datasets[2]

        # compute number of minibatches for training, validation and testing
        n_valid_batches = valid_set_x.value.shape[0] / batch_size
        n_test_batches  = test_set_x.value.shape[0]  / batch_size

        index   = T.lscalar('index')    # index to a [mini]batch 

        # compute the gradients with respect to the model parameters
        gparams = T.grad(self.finetune_cost, self.params)

        # compute list of fine-tuning updates
        updates = {}
        for param, gparam in zip(self.params, gparams):
            updates[param] = param - gparam*learning_rate

        train_fn = theano.function(inputs = [index], 
              outputs =   self.finetune_cost, 
              updates = updates,
              givens  = {
                self.x : train_set_x[index*batch_size:(index+1)*batch_size],
                self.y : train_set_y[index*batch_size:(index+1)*batch_size]})

        test_score_i = theano.function([index], self.errors,
                 givens = {
                   self.x: test_set_x[index*batch_size:(index+1)*batch_size],
                   self.y: test_set_y[index*batch_size:(index+1)*batch_size]})

        valid_score_i = theano.function([index], self.errors,
              givens = {
                 self.x: valid_set_x[index*batch_size:(index+1)*batch_size],
                 self.y: valid_set_y[index*batch_size:(index+1)*batch_size]})

        # Create a function that scans the entire validation set
        def valid_score():
            return [valid_score_i(i) for i in xrange(n_valid_batches)]

        # Create a function that scans the entire test set
        def test_score():
            return [test_score_i(i) for i in xrange(n_test_batches)]

        return train_fn, valid_score, test_score


Note that the returned ``valid_score`` and ``test_score`` are not Theano
functions, but rather python functions that also loop over the entire 
validation set and the entire test set producing a list of the losses
over these sets.


Putting it all together
+++++++++++++++++++++++

The few lines of code below constructs the deep belief network : 

.. code-block:: python 

    numpy_rng = numpy.random.RandomState(123)
    print '... building the model'
    # construct the Deep Belief Network
    dbn = DBN(numpy_rng = numpy_rng, n_ins = 28*28, 
              hidden_layers_sizes = [1000,1000,1000],
              n_outs = 10)
    


There are two stages in training this network, a layer-wise pre-training and 
fine-tuning afterwards. 

For the pre-training stage, we will loop over all the layers of the
network. For each layer we will use the compiled theano function that
implements a SGD step towards optimizing the weights for reducing 
the reconstruction cost of that layer. This function will be applied 
to the training set for a fixed number of epochs given by
``pretraining_epochs``.


.. code-block:: python


    #########################
    # PRETRAINING THE MODEL #
    #########################
    print '... getting the pretraining functions'
    pretraining_fns = dbn.pretraining_functions(
            train_set_x   = train_set_x, 
            batch_size    = batch_size ) 

    print '... pre-training the model'
    start_time = time.clock()  
    ## Pre-train layer-wise 
    for i in xrange(dbn.n_layers):
        # go through pretraining epochs 
        for epoch in xrange(pretraining_epochs):
            # go through the training set
            c = []
            for batch_index in xrange(n_train_batches):
                c.append(pretraining_fns[i](index = batch_index, 
                         lr = pretrain_lr ) )
            print 'Pre-training layer %i, epoch %d, cost '%(i,epoch),numpy.mean(c)
 
    end_time = time.clock()

The fine-tuning loop is very similar with the one in the :ref:`mlp`, the
only difference is that we will use now the functions given by
`build_finetune_functions`.

Running the Code
++++++++++++++++

The user can run the code by calling:

.. code-block:: bash
  
  python code/DBN.py


Sampling a DBN
++++++++++++++

