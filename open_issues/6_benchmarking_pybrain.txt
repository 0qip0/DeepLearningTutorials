Reported by : Razvan

Observations : 

    1.  First thing, working with their dataset model is a pain ! Either I had 
        not figure it out, or it allows you to add only one datapoint at a time 
        in the dataset. This seems to me highly unoptimal ...

    2.  You do not get batches for sgd ! The only thing you can do is compare with 
        batch size of 1.

    3.  Their early stopping is different from ours. Differences : 
            - You can not set how often you do a pass on the validation set 
              (i.e. ``patience`` in our case).  You always do one epoch of training 
              and then you go through the validation set.
            - You do not have an improvement thereshold, any improvement in
              validation score leads to storing the new best parameters, and
              increasing the time you will still look for better parameters
            - The increase is not by multiplication but summation. So if at
              epoch x you do better on the validation step, you will go on for 
              x+y epochs to look for something better ( we do x*y )

    4.  The errors return by pyBrain are divided by the number of
        classes. So if you do classification, you take the number of
        errors and divide it by the number of test examples times the
        number of classes. For MNIST this yields 10 times smaller
        errors. Is this something standard .. should we do it ? It
        definetelly makes error look smaller.

    5.  There is no straight forward way of adding L1/L2 regularization (from
        what I've seen), unless you go into their code and change it. That is not
        ard to do .. but for now I do not want to meangle with the library

    6.  The code for RBM is not ready (they say that it is work in progress). It seems to me that the 
        code is wrong .. They have 3 loops, which to me would mean that the inner most is for CD-k (
        second is for one epoch / third for training). But they update the weights after each Gibbs 
        step in CD-k .. which results in a strage form of CD-1 that sees same example several time before
        moving to the next one. I could (?) potentially fix the code but it is outside the scope of 
        benchmarking. 

    7.  There are question marks of how easy it would be to implement a SdA ( autoassociators might be 
        easy to do though).


    RESULTS : 
    logreg on maggie46

    *** Using their early stopping :  ***
    Only training took :  7221.22 (s) = 120.353 min for 150 epochs = > 48.141s/epoch
    A pass through the test takes: 2.8s
    Total amount of time spend: 7224.02
    Final error is : 7.77

    ** Our thing with batchsize =1 **
    439s for 30 epochs => 14.63s
    our thing with batchsize 600 :0.43s


    Results : 
    mlp on maggie46
    *** Using their early stopping :  ***
    Only training took :  5317.4(s) = 88.623 min for 3 epochs => 29.5 min/epoch
    A pass through the test takes: 7.51s
    Total amount of time spend: 5324.91
    Final error is : 4.87 ( This was run only for 3 epochs .. because it was
    to slow .. a different run that I did not allow to end seems to suggest that 
    they are getting similar final testing performance as us)
    
    similar parametrs for 3 epochs we get : 
    Optimization complete. Best validation score of 3.600000 % obtained at
    iteration 0, with test performance 3.880000 %
    The code for file mlp.py ran for 13.12m expected 1.51m in our buildbot =>
    4.37min / epoch
    out thing with batchsize 20 :1.78min/epoch



